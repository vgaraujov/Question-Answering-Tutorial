{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Question Answering with BERT Spanish.ipynb","provenance":[{"file_id":"https://github.com/spark-ming/albert-qa-demo/blob/master/Question_Answering_with_ALBERT.ipynb","timestamp":1598458028761}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1qfQAtRsMVl7","colab_type":"text"},"source":["# Question Answering with BERT and SQuAD Spanish (using Hugging Face)\n","\n","Author: Vladimir Araujo\n","\n","Based on: https://www.spark64.com/post/machine-comprehension\n","\n","## Introduction\n","\n","Question Answering (QA) is a challenging task that NLP tries to solve. The aim is to provide solution to queries expressed in natural language automatically (Hovy, Gerber, Hermjakob, Junk, and Lin 2000). For instance, given the following context:\n","\n","> Quito, oficialmente San Francisco de Quito, es la capital de la República del Ecuador, de la Provincia de Pichincha y la capital más antigua de Sudamérica. Es la ciudad más poblada del Ecuador,​ con 2 millones de habitantes en el área urbana, y aproximadamente 3 millones en todo el Área metropolitana.\n","\n","We ask the question\n","\n","> ¿Cuál es la población de Quito?\n","\n","We expect the QA system responds with something like this:\n","\n","> 2 millones\n","\n","Since 2017, transformer models have been shown to outperform existing approaches for this task. Currently, many pretrained transformer models exist, including BERT, GPT-2, XLNet.\n","\n","This tutorial shows how you can fine-tune BERT for the task of QA and use it for inference. We will use the transformer library built by [Hugging Face](https://huggingface.co/), which is an extremely useful implementation of the transformer models in both TensorFlow and PyTorch. You can just use a fine-tuned model from their [model hub](https://huggingface.co/models). \n"," \n","This tutorial is for educational purposes with which we will learn to finetune a BERT model and use it with your own data.\n","\n","## Using BERT-based model for QA\n","\n","<figure>\n","<center>\n","<img src='https://miro.medium.com/max/1840/1*QhIXsDBEnANLXMA0yONxxA.png' width=\"500\" />\n","</center>\n","</figure>\n","\n","*   Represent the question using the $A$ embedding and the paragraph using the $B$ embedding. \n","*   New parameters learned during fine-tuning are a start vector $S$ and an end vector $E$.\n","*   The probability of word $i$ being the start/end of the answer span is computed as a dot product between $T_{i}$ and $S$ or $E$ followed by a softmax. Where $T_{i}$ is BERT final hidden vector."]},{"cell_type":"markdown","metadata":{"id":"z45N5VRcvrjD","colab_type":"text"},"source":["# Nueva sección"]},{"cell_type":"markdown","metadata":{"id":"sBBHbGvQN5vX","colab_type":"text"},"source":["## 1.0 Setup\n","\n","Let's check out what kind of GPU is assigned. This notebook should be configured to give you a P100 (saved in metadata)."]},{"cell_type":"code","metadata":{"id":"frTeTcy4WdbY","colab_type":"code","colab":{}},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5RImM3oWbrZ","colab_type":"text"},"source":["First, we clone and install the Hugging Face transformer library from Github."]},{"cell_type":"code","metadata":{"id":"QOAoUwBFMQCg","colab_type":"code","colab":{}},"source":["!git clone https://github.com/huggingface/transformers \\\n","&& cd transformers \\\n","&& git checkout a3085020ed0d81d4903c50967687192e3101e770 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TRZned-8WJrj","colab_type":"code","colab":{}},"source":["!pip install ./transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UHCuzhPptH0M","colab_type":"text"},"source":["## 2.0 Train Model\n","\n","This is where we can train our own model."]},{"cell_type":"markdown","metadata":{"id":"OaQGsAiWXcnd","colab_type":"text"},"source":["### 2.1 Get Training and Evaluation Data\n","\n","The [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. In this tutorial we will use a Spanish version of this dataset. \n","\n","Read more about this dataset here: https://github.com/ccasimiro88/TranslateAlignRetrieve\n","\n","Now get the SQuAD V2.0 dataset. `train-v2.0-es_small.json\n","` is for training and `dev-v2.0-es_small.json` is for evaluation to see how well your model trained. Note that we use the small version for convenience.\n","\n"]},{"cell_type":"code","metadata":{"id":"4Qivanca5hdf","colab_type":"code","colab":{}},"source":["!mkdir dataset \\\n","&& cd dataset \\\n","&& wget https://github.com/ccasimiro88/TranslateAlignRetrieve/raw/master/SQuAD-es-v2.0/train-v2.0-es_small.json \\\n","&& wget https://github.com/ccasimiro88/TranslateAlignRetrieve/raw/master/SQuAD-es-v2.0/dev-v2.0-es_small.json"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dZ87q93GDeeL","colab_type":"text"},"source":["### 2.2 Run training (Optional)\n","\n","We can now train the model with the training set. \n","\n","### Notes about parameters:\n","`per_gpu_train_batch_size` specifies the number of training examples per iteration per GPU.\n","\n","`save_steps` specifies number of steps before it outputs a checkpoint file. I've increased it to save disk space.\n","\n","`num_train_epochs` sets the number of epochs, two epochs are recommended. It's currently set to one for the purpose of time.\n","\n","`version_2_with_negative` is required for SQuAD V2.0. If training with V1.1, take out this flag.\n","\n","NOTE: it takes about 1 hour to train an epoch! If you don't want to wait this long, feel free to skip this step and note the comment in the code to use a pretrained model!"]},{"cell_type":"code","metadata":{"id":"Z75F_lxhIk6m","colab_type":"code","colab":{}},"source":["!export SQUAD_DIR=/content/dataset \\\n","&& python transformers/examples/run_squad.py \\\n","  --model_type bert \\\n","  --model_name_or_path dccuchile/bert-base-spanish-wwm-cased \\\n","  --do_train \\\n","  --do_eval \\\n","  --do_lower_case \\\n","  --train_file $SQUAD_DIR/train-v2.0-es_small.json \\\n","  --predict_file $SQUAD_DIR/dev-v2.0-es_small.json \\\n","  --per_gpu_train_batch_size 12 \\\n","  --learning_rate 3e-5 \\\n","  --num_train_epochs 2.0 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --output_dir /content/model_output \\\n","  --save_steps 5000 \\\n","  --threads 4 \\\n","  --version_2_with_negative "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-JCNRkQwUD56","colab_type":"text"},"source":["## 3.0 Setup prediction code\n","\n","Now we can use the Hugging Face library to make predictions using our newly trained model. Note that a lot of the code is pulled from `run_squad.py` in the Hugging Face repository, with all the training parts removed. This modified code allows to run predictions we pass in directly as strings, rather .json format like the training/test set.\n","\n","NOTE: if you decided train your own mode, change the flag `use_own_model` to `True`.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","cellView":"code","id":"9jZkAMli9tot","colab":{}},"source":["import os\n","import torch\n","import time\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","from transformers import (\n","    AutoTokenizer, \n","    AutoModelForQuestionAnswering,\n","    squad_convert_examples_to_features\n",")\n","\n","from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n","\n","from transformers.data.metrics.squad_metrics import compute_predictions_logits\n","\n","# READER NOTE: Set this flag to use own model, or use pretrained model in the Hugging Face repository\n","use_own_model = False\n","\n","if use_own_model:\n","  model_name_or_path = \"/content/model_output\"\n","else:\n","  model_name_or_path = \"mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\"\n","\n","output_dir = \"\"\n","\n","# Config\n","n_best_size = 1\n","max_answer_length = 30\n","do_lower_case = True\n","null_score_diff_threshold = 0.0\n","\n","def to_list(tensor):\n","    return tensor.detach().cpu().tolist()\n","\n","# Setup model\n","model_class, tokenizer_class = (AutoModelForQuestionAnswering, AutoTokenizer)\n","tokenizer = tokenizer_class.from_pretrained(\n","    model_name_or_path, do_lower_case=True)\n","model = model_class.from_pretrained(model_name_or_path)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model.to(device)\n","\n","processor = SquadV2Processor()\n","\n","def run_prediction(question_texts, context_text):\n","    \"\"\"Setup function to compute predictions\"\"\"\n","    examples = []\n","\n","    for i, question_text in enumerate(question_texts):\n","        example = SquadExample(\n","            qas_id=str(i),\n","            question_text=question_text,\n","            context_text=context_text,\n","            answer_text=None,\n","            start_position_character=None,\n","            title=\"Predict\",\n","            is_impossible=False,\n","            answers=None,\n","        )\n","\n","        examples.append(example)\n","\n","    features, dataset = squad_convert_examples_to_features(\n","        examples=examples,\n","        tokenizer=tokenizer,\n","        max_seq_length=384,\n","        doc_stride=128,\n","        max_query_length=64,\n","        is_training=False,\n","        return_dataset=\"pt\",\n","        threads=1,\n","    )\n","\n","    eval_sampler = SequentialSampler(dataset)\n","    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n","\n","    all_results = []\n","\n","    for batch in eval_dataloader:\n","        model.eval()\n","        batch = tuple(t.to(device) for t in batch)\n","\n","        with torch.no_grad():\n","            inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"token_type_ids\": batch[2],\n","            }\n","\n","            example_indices = batch[3]\n","\n","            outputs = model(**inputs)\n","\n","            for i, example_index in enumerate(example_indices):\n","                eval_feature = features[example_index.item()]\n","                unique_id = int(eval_feature.unique_id)\n","\n","                output = [to_list(output[i]) for output in outputs]\n","\n","                start_logits, end_logits = output\n","                result = SquadResult(unique_id, start_logits, end_logits)\n","                all_results.append(result)\n","\n","    output_prediction_file = \"predictions.json\"\n","    output_nbest_file = \"nbest_predictions.json\"\n","    output_null_log_odds_file = \"null_predictions.json\"\n","\n","    predictions = compute_predictions_logits(\n","        examples,\n","        features,\n","        all_results,\n","        n_best_size,\n","        max_answer_length,\n","        do_lower_case,\n","        output_prediction_file,\n","        output_nbest_file,\n","        output_null_log_odds_file,\n","        False,  # verbose_logging\n","        True,  # version_2_with_negative\n","        null_score_diff_threshold,\n","        tokenizer,\n","    )\n","\n","    return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nIQOB8vhpcKs","colab_type":"text"},"source":["## 4.0 Run predictions\n","\n","Now for the fun part... testing out your model on different inputs. Pretty rudimentary example here. But the possibilities are endless with this function."]},{"cell_type":"code","metadata":{"id":"F-sUrcA5nXTH","colab_type":"code","cellView":"code","colab":{}},"source":["context = \"Quito, oficialmente San Francisco de Quito, es la capital de la República del Ecuador, de la Provincia de Pichincha y la capital más antigua de Sudamérica. Es la ciudad más poblada del Ecuador,​ con 2 millones de habitantes en el área urbana, y aproximadamente 3 millones en todo el Área metropolitana.\"\n","questions = [\"¿Cuál es la población de Quito?\", \n","             \"¿En qué provincia esta ubicado Quito?\"]\n","\n","# Run method\n","predictions = run_prediction(questions, context)\n","\n","# Print results\n","print(\"Results:\")\n","for i, key in enumerate(predictions.keys()):\n","  print(questions[i],predictions[key])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rkivu8FOqp_8","colab_type":"text"},"source":["## 5.0 Activity\n","\n","Now is your turn. Use the code in Section 4.0 (previous section) to generate your own predictions. To do that, you must change the context variables and questions.\n"]},{"cell_type":"code","metadata":{"id":"dmf_DUvufzRc","colab_type":"code","colab":{}},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C7x2k-5DPofz","colab_type":"text"},"source":["\n","---\n","\n","Base on this tutorial and the class, answer the following questions with `Yes` or `No`.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3gNeQbJoQh4q","colab_type":"text"},"source":["*   Is the SQuAD dataset a reading comprehension task?: `Your answer here`\n","*   Is the BERT model trained from scratch for the QA task?: `Your answer here`\n","*   Is this model predicting the position ot the answer in the context phrase? `Your answer here`\n","\n","\n"]}]}